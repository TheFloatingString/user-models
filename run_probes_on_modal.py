"""
Script using Modal, OpenRouter, and TransformerLens for mech interp:
1. GPT-4o generates conversations as personas with demographics
2. Gemma-2-9b-it responds and we extract activations
3. Train linear probes to detect demographic info in activations
4. Evaluate what demographic features are encoded in the model
5. Log everything to Weights & Biases
"""

import modal
import os
import json

# Define Modal image with dependencies
image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "transformer-lens>=2.0.0",
    "torch>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "matplotlib>=3.7.0",
    "tqdm>=4.65.0",
    "pandas>=2.0.0",
    "openai>=1.0.0",
    "python-dotenv>=1.0.0",
    "wandb>=0.16.0",
)

app = modal.App("gemma-linear-probes")

# Create Modal volume for storing results
volume = modal.Volume.from_name("probe-results", create_if_missing=True)

VOLUME_PATH = "/data"

USER_MODEL = "openai/gpt-4o"
ASSISTANT_MODEL = "google/gemma-2-9b-it"


@app.function(
    image=image,
    secrets=[modal.Secret.from_name("openrouter-secret")],
    timeout=600,
)
def generate_conversation_with_persona(
    age_range: str,
    income_range: str,
    education: str,
    sex: str,
    visa_status: str,
    num_turns: int = 3,
) -> list[dict]:
    """Generate conversation using GPT-4o as persona.

    Args:
        age_range: Age range of persona
        income_range: Income range of persona
        education: Education level
        sex: Sex of persona
        visa_status: Visa status
        num_turns: Number of conversation turns

    Returns:
        List of conversation messages
    """
    from openai import OpenAI

    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.environ.get("OPENROUTER_API_KEY"),
    )

    # System prompt for persona
    persona_prompt = f"""You are a person with these demographics:
- Age: {age_range}
- Income: {income_range}
- Education: {education}
- Sex: {sex}
- Visa Status: {visa_status}

Engage naturally in conversation. Let your demographics
influence your perspective and knowledge, but don't explicitly
state them."""

    messages = []
    conversation = [{"role": "system", "content": persona_prompt}]

    # Generate conversation turns
    topics = [
        "Tell me about your typical day.",
        "What are your thoughts on current technology?",
        "What are your future plans or goals?",
    ]

    for i in range(min(num_turns, len(topics))):
        # Assistant asks question
        assistant_msg = topics[i]
        conversation.append({"role": "assistant", "content": assistant_msg})

        # User (GPT-4o persona) responds
        response = client.chat.completions.create(
            model=USER_MODEL,
            messages=conversation,
        )
        user_msg = response.choices[0].message.content
        conversation.append({"role": "user", "content": user_msg})

        messages.append({"role": "assistant", "content": assistant_msg})
        messages.append({"role": "user", "content": user_msg})

    return messages


@app.function(
    image=image,
    gpu="A10G",
    timeout=1800,
    secrets=[
        modal.Secret.from_name("openrouter-secret"),
        modal.Secret.from_name("huggingface-secret"),
    ],
    volumes={VOLUME_PATH: volume},
)
def extract_activations_from_responses(
    conversations: list[dict],
    layer_idx: int,
) -> tuple[list, list]:
    """Extract Gemma activations from assistant responses.

    Args:
        conversations: List of conversation dicts
        layer_idx: Layer to extract activations from

    Returns:
        Tuple of (activations, labels)
    """
    import torch
    from transformer_lens import HookedTransformer

    print("Loading Gemma model...")
    model = HookedTransformer.from_pretrained(
        "gemma-2-9b",
        device="cuda" if torch.cuda.is_available() else "cpu",
        hf_token=os.environ.get("HF_TOKEN"),
    )

    activations = []
    labels = []

    for conv_data in conversations:
        conversation = conv_data["conversation"]
        persona = conv_data["persona"]

        # Extract user responses (generated by GPT-4o persona)
        user_texts = [msg["content"] for msg in conversation if msg["role"] == "user"]

        # Get activations for each user response
        for text in user_texts:
            with torch.no_grad():
                _, cache = model.run_with_cache(text)
                # Extract from specified layer
                layer_acts = cache[f"blocks.{layer_idx}.hook_resid_post"]
                # Mean pool over sequence
                pooled = layer_acts.mean(dim=1).cpu().numpy()[0]
                activations.append(pooled)

        # Store labels (demographics)
        labels.append(
            {
                "age_range": persona.get("age_range", ""),
                "income_range": persona.get("income_range", ""),
                "education": persona.get("education", ""),
                "sex": persona.get("sex", ""),
                "visa_status": persona.get("visa_status", ""),
            }
        )

    return activations, labels


@app.function(
    image=image,
    timeout=600,
    secrets=[modal.Secret.from_name("wandb-secret")],
)
def train_probe_for_feature(
    activations: list,
    labels: list[dict],
    feature_name: str,
    run_name: str,
) -> dict:
    """Train linear probe for specific demographic feature.

    Args:
        activations: List of activation arrays
        labels: List of label dicts
        feature_name: Feature to probe for
        run_name: W&B run name

    Returns:
        Probe results with accuracy metrics
    """
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, confusion_matrix
    from sklearn.preprocessing import LabelEncoder
    import numpy as np
    import wandb

    # Extract feature labels
    y_raw = [label[feature_name] for label in labels]

    # Encode labels
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y_raw)

    # Repeat activations for each conversation turn
    X = []
    y_expanded = []
    for i, acts in enumerate(activations):
        # Handle multiple turns
        if isinstance(acts, list):
            for act in acts:
                X.append(act)
                y_expanded.append(y[i])
        else:
            X.append(acts)
            y_expanded.append(y[i])

    X = np.array(X)
    y_expanded = np.array(y_expanded)

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_expanded, test_size=0.2, random_state=42
    )

    # Train probe
    probe = LogisticRegression(max_iter=1000, random_state=42, class_weight="balanced")
    probe.fit(X_train, y_train)

    # Evaluate
    y_pred = probe.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    print(f"{feature_name} probe accuracy: {accuracy:.3f}")

    # Log to W&B
    wandb.log(
        {
            f"{feature_name}/accuracy": accuracy,
            f"{feature_name}/n_samples": len(X),
            f"{feature_name}/n_train": len(X_train),
            f"{feature_name}/n_test": len(X_test),
        }
    )

    return {
        "feature": feature_name,
        "accuracy": accuracy,
        "classes": label_encoder.classes_.tolist(),
        "n_samples": len(X),
        "confusion_matrix": conf_matrix.tolist(),
    }


@app.function(
    image=image,
    gpu="A10G",
    timeout=3600,
    secrets=[
        modal.Secret.from_name("openrouter-secret"),
        modal.Secret.from_name("wandb-secret"),
    ],
    volumes={VOLUME_PATH: volume},
)
def run_probe_experiment(
    personas: list[dict],
    layer_idx: int = 20,
    project_name: str = "gemma-probes",
    run_name: str = "probe-experiment",
) -> dict:
    """Run full probe experiment on personas.

    Args:
        personas: List of persona dicts
        layer_idx: Layer to probe (default: 20)
        project_name: W&B project name
        run_name: W&B run name

    Returns:
        Experiment results
    """
    import wandb

    # Initialize W&B
    wandb.init(
        project=project_name,
        name=run_name,
        config={
            "layer_idx": layer_idx,
            "num_personas": len(personas),
            "user_model": USER_MODEL,
            "assistant_model": ASSISTANT_MODEL,
        },
    )

    print(f"Running probe experiment on {len(personas)} personas")
    print(f"Target layer: {layer_idx}")
    print(f"W&B run: {wandb.run.url}")

    # Generate conversations in parallel
    print("\n" + "=" * 50)
    print("GENERATING CONVERSATIONS (PARALLEL)")
    print("=" * 50)

    # Start all conversation generations in parallel
    print(f"Starting {len(personas)} conversations in parallel...")
    futures = []
    for persona in personas:
        future = generate_conversation_with_persona.spawn(
            age_range=persona.get("age_range", ""),
            income_range=persona.get("income_range", ""),
            education=persona.get("education", ""),
            sex=persona.get("sex", ""),
            visa_status=persona.get("visa_status", ""),
        )
        futures.append((persona, future))

    # Collect results as they complete
    conversations = []
    for i, (persona, future) in enumerate(futures):
        print(f"Waiting for persona {i + 1}/{len(futures)}...")
        conv = future.get()
        conversations.append(
            {
                "persona": persona,
                "conversation": conv,
            }
        )

    print(f"Completed all {len(conversations)} conversations")
    wandb.log({"num_conversations": len(conversations)})

    # Extract activations
    print("\n" + "=" * 50)
    print("EXTRACTING ACTIVATIONS")
    print("=" * 50)

    activations, labels = extract_activations_from_responses.remote(
        conversations, layer_idx
    )

    wandb.log({"num_activations": len(activations)})

    # Train probes for each feature
    print("\n" + "=" * 50)
    print("TRAINING PROBES")
    print("=" * 50)

    features = ["age_range", "income_range", "education", "sex"]
    results = []

    for feature in features:
        print(f"\nTraining probe for: {feature}")
        result = train_probe_for_feature.remote(
            activations,
            labels,
            feature,
            run_name,
        )
        results.append(result)

    # Log summary
    avg_accuracy = sum(r["accuracy"] for r in results) / len(results)
    wandb.log({"average_accuracy": avg_accuracy})

    wandb.finish()

    return {
        "layer_idx": layer_idx,
        "num_personas": len(personas),
        "probe_results": results,
        "average_accuracy": avg_accuracy,
    }


@app.local_entrypoint()
def main(
    personas_file: str = "data/personas.json",
    layer_idx: int = 20,
    output_file: str = "data/results/probe_results.json",
    wandb_project: str = "gemma-probes",
    wandb_run: str = "probe-experiment",
):
    """Main entrypoint for running probes on Modal.

    Args:
        personas_file: Path to personas JSON
        layer_idx: Layer to probe
        output_file: Output results path
        wandb_project: W&B project name
        wandb_run: W&B run name
    """
    print("=" * 60)
    print("GEMMA LINEAR PROBES - MECHANISTIC INTERPRETABILITY")
    print("=" * 60)
    print(f"Layer: {layer_idx}")
    print(f"Personas: {personas_file}")
    print(f"W&B Project: {wandb_project}")

    # Load personas
    with open(personas_file) as f:
        data = json.load(f)
        personas = data.get("personas", [])

    print(f"\nLoaded {len(personas)} personas")

    # Run experiment
    results = run_probe_experiment.remote(
        personas=personas,
        layer_idx=layer_idx,
        project_name=wandb_project,
        run_name=wandb_run,
    )

    # Save results
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)
    for probe_result in results["probe_results"]:
        feature = probe_result["feature"]
        acc = probe_result["accuracy"]
        print(f"{feature:15s}: {acc:.3f}")

    print(f"\nAverage accuracy: {results['average_accuracy']:.3f}")
    print(f"\nResults saved to: {output_file}")
    print("\nCompleted!")

[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
============================================================
REFUSAL RATE FLUENCY EXPERIMENT
============================================================

Created 42 personas
Loaded 24 questions

============================================================
SINGLE-TURN EXPERIMENTS
============================================================
Traceback (most recent call last):
  File "C:\Users\laure\Projects\user-models\run_refusal_experiment.py", line 370, in <module>
    main()
  File "C:\Users\laure\Projects\user-models\run_refusal_experiment.py", line 345, in main
    all_results.update(run_single_turn_experiments(client, personas, questions, wandb))
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\run_refusal_experiment.py", line 88, in run_single_turn_experiments
    conv = generate_single_turn_conversation(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\src\refusal_conversation.py", line 38, in generate_single_turn_conversation
    target_response = client.chat.completions.create(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\openai\_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_client.py", line 928, in send
    raise exc
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_client.py", line 922, in send
    response.read()
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_models.py", line 881, in read
    self._content = b"".join(self.iter_bytes())
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_models.py", line 897, in iter_bytes
    for raw_bytes in self.iter_raw():
                     ^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_models.py", line 951, in iter_raw
    for raw_stream_bytes in self.stream:
                            ^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_client.py", line 153, in __iter__
    for chunk in self._stream:
                 ^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpx\_transports\default.py", line 127, in __iter__
    for part in self._httpcore_stream:
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 407, in __iter__
    raise exc from None
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 403, in __iter__
    for part in self._stream:
                ^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 342, in __iter__
    raise exc
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 334, in __iter__
    for chunk in self._connection._receive_response_body(**kwargs):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 203, in _receive_response_body
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\Projects\user-models\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\AppData\Roaming\uv\python\cpython-3.12.7-windows-x86_64-none\Lib\ssl.py", line 1232, in recv
    return self.read(buflen)
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\laure\AppData\Roaming\uv\python\cpython-3.12.7-windows-x86_64-none\Lib\ssl.py", line 1105, in read
    return self._sslobj.read(len)
           ^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
